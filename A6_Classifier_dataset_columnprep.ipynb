{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import html\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import os\n",
    "# TQDM to Show Progress Bars #\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "# SKLearn libraries for splitting sample and validation\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jy/j77v476x03737ryp7gll_0p00000gn/T/ipykernel_28331/1989358710.py:1: DtypeWarning: Columns (18,20,21,23,49,50) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  patents_text = pd.read_csv(\"patents_text_deduplicated.csv\")\n"
     ]
    }
   ],
   "source": [
    "patents_text = pd.read_csv(\"patents_text_deduplicated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data = pd.read_csv(\"labelled_patents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data = pd.read_csv(\"unseen_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep for Classifier\n",
    "Step 1: \n",
    "Only use titles and abstracts\n",
    "Use Scibert\n",
    "5 fold cross validation\n",
    "Start with Narrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['publication_number_EPO', 'AI for Sus', 'Sus of AI', 'AI ', 'Sus',\n",
       "       'Energy grid/production', 'Electronic component/battery/charging',\n",
       "       'Example patent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = labelled_data[[\"publication_number_EPO\", \"Sus of AI\", \"AI for Sus\"]]\n",
    "training_data = training_data.merge(patents_text[['publication_number_EPO', 'abstract', 'title', 'description', 'claims', 'cpc_codes_EPO']], on='publication_number_EPO', how='left')\n",
    "training_data['title'] = training_data['title'].fillna('')\n",
    "training_data['abstract'] = training_data['abstract'].fillna('')\n",
    "training_data['description'] = training_data['description'].fillna('')\n",
    "training_data['claims'] = training_data['claims'].fillna('')\n",
    "\n",
    "# Function to extract first 200 words from a text\n",
    "def extract_first_300_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return ' '.join(words[:300])\n",
    "\n",
    "def extract_first_3000_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return ' '.join(words[:3000])\n",
    "\n",
    "# Apply the function to each row in training_data\n",
    "\n",
    "# Concatenate \"title\" and \"abstract\" columns, replacing NaN with empty string\n",
    "training_data[\"text_abstr\"] = (training_data[\"title\"] + \" \" + training_data[\"abstract\"]).fillna('')\n",
    "training_data[\"text_descr300\"] = (training_data[\"title\"] + \" \" + training_data[\"abstract\"] + \" \" + training_data[\"description\"].fillna('').apply(extract_first_300_words))\n",
    "training_data[\"text_descr\"] = (training_data[\"title\"] + \" \" + training_data[\"abstract\"] + \" \" + training_data[\"description\"]).fillna('')\n",
    "training_data[\"text_descr_claims\"] = (training_data[\"title\"] + \" \" + training_data[\"abstract\"] + \" \" + training_data[\"description\"] + \" \" + training_data[\"claims\"]).fillna('')\n",
    "training_data[\"text_descr3000_claims\"] = (training_data[\"title\"] + \" \" + training_data[\"abstract\"] + \" \" + training_data[\"description\"].fillna('').apply(extract_first_3000_words)) + \" \" + training_data[\"claims\"].fillna('')\n",
    "training_data[\"text_claims\"] = (training_data[\"title\"] + \" \" + training_data[\"abstract\"] + \" \" + training_data[\"claims\"]).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_abstr: 205.73926073926074 words\n",
      "text_descr300: 426.01898101898104 words\n",
      "text_descr: 9074.454545454546 words\n",
      "text_descr_claims: 9887.696303696304 words\n",
      "text_descr3000_claims: 3151.2607392607392 words\n",
      "text_claims: 1018.981018981019 words\n",
      "text_abstr: 211.19727891156464 words\n",
      "text_descr300: 511.19727891156464 words\n",
      "text_descr: 12289.542857142857 words\n",
      "text_descr_claims: 13397.10068027211 words\n",
      "text_descr3000_claims: 4222.717006802721 words\n",
      "text_claims: 1318.7551020408164 words\n"
     ]
    }
   ],
   "source": [
    "# print average length of text in words for each column\n",
    "for col in [\"text_abstr\", \"text_descr300\", \"text_descr\", \"text_descr_claims\", \"text_descr3000_claims\", \"text_claims\"]:\n",
    "    print(f\"{col}: {training_data[col].str.split().apply(len).mean()} words\")\n",
    "\n",
    "# check average length of text in words for each column taking only rows with claims and description not being null strings\n",
    "for col in [\"text_abstr\", \"text_descr300\", \"text_descr\", \"text_descr_claims\", \"text_descr3000_claims\", \"text_claims\"]:\n",
    "    print(f\"{col}: {training_data[(training_data['claims'] != '') & (training_data['description'] != '')][col].str.split().apply(len).mean()} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[\"SofAI\"] = training_data[\"Sus of AI\"]\n",
    "training_data[\"AIforS\"] = training_data[\"AI for Sus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv(\"training_data_new3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep for Multimodal Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['B01D 53/0476', 'B01D 53/0476', 'B01D 53/0431...\n",
       "1    ['B60K 6/00', 'B60K 6/48', 'B60K 6/485', 'B60K...\n",
       "2    ['G06F 13/387', 'G06F 13/387', 'H04W 4/80', 'H...\n",
       "3    ['H02J 13/00016', 'H02J 13/00016', 'Y02E 60/00...\n",
       "4    ['H02M 1/10', 'H02M 1/10', 'H02M 3/33592', 'H0...\n",
       "Name: cpc_codes_EPO, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patents_text[\"cpc_codes_EPO\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "5\n",
      "3\n",
      "/\n",
      "0\n",
      "4\n",
      "7\n",
      "6\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "5\n",
      "3\n",
      "/\n",
      "0\n",
      "4\n",
      "7\n",
      "6\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "5\n",
      "3\n",
      "/\n",
      "0\n",
      "4\n",
      "3\n",
      "1\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "5\n",
      "3\n",
      "/\n",
      "0\n",
      "4\n",
      "3\n",
      "1\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "5\n",
      "3\n",
      "/\n",
      "0\n",
      "4\n",
      "4\n",
      "6\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "5\n",
      "3\n",
      "/\n",
      "0\n",
      "4\n",
      "4\n",
      "6\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "2\n",
      "2\n",
      "5\n",
      "6\n",
      "/\n",
      "1\n",
      "2\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "2\n",
      "2\n",
      "5\n",
      "6\n",
      "/\n",
      "1\n",
      "2\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "2\n",
      "2\n",
      "5\n",
      "7\n",
      "/\n",
      "1\n",
      "0\n",
      "2\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "2\n",
      "2\n",
      "5\n",
      "7\n",
      "/\n",
      "1\n",
      "0\n",
      "2\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "2\n",
      "2\n",
      "5\n",
      "9\n",
      "/\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "7\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "2\n",
      "2\n",
      "5\n",
      "9\n",
      "/\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "7\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "2\n",
      "2\n",
      "5\n",
      "9\n",
      "/\n",
      "4\n",
      "0\n",
      "1\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "2\n",
      "2\n",
      "5\n",
      "9\n",
      "/\n",
      "4\n",
      "0\n",
      "1\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "2\n",
      "2\n",
      "5\n",
      "9\n",
      "/\n",
      "4\n",
      "0\n",
      "2\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "2\n",
      "2\n",
      "5\n",
      "9\n",
      "/\n",
      "4\n",
      "0\n",
      "2\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "2\n",
      "2\n",
      "5\n",
      "9\n",
      "/\n",
      "4\n",
      "0\n",
      "3\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "0\n",
      "1\n",
      "D\n",
      " \n",
      "2\n",
      "2\n",
      "5\n",
      "9\n",
      "/\n",
      "4\n",
      "0\n",
      "3\n",
      "'\n",
      "]\n",
      "[\n",
      "'\n",
      "B\n",
      "6\n",
      "0\n",
      "K\n",
      " \n",
      "6\n",
      "/\n",
      "0\n",
      "0\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "6\n",
      "0\n",
      "K\n",
      " \n",
      "6\n",
      "/\n",
      "4\n",
      "8\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "6\n",
      "0\n",
      "K\n",
      " \n",
      "6\n",
      "/\n",
      "4\n",
      "8\n",
      "5\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "6\n",
      "0\n",
      "K\n",
      " \n",
      "6\n",
      "/\n",
      "5\n",
      "4\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "6\n",
      "0\n",
      "W\n",
      " \n",
      "1\n",
      "0\n",
      "/\n",
      "0\n",
      "2\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "6\n",
      "0\n",
      "W\n",
      " \n",
      "1\n",
      "0\n",
      "/\n",
      "0\n",
      "6\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "6\n",
      "0\n",
      "W\n",
      " \n",
      "1\n",
      "0\n",
      "/\n",
      "0\n",
      "8\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "B\n",
      "6\n",
      "0\n",
      "W\n",
      " \n",
      "1\n",
      "0\n",
      "/\n",
      "1\n",
      "0\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "Y\n",
      "0\n",
      "2\n",
      "T\n",
      " \n",
      "1\n",
      "0\n",
      "/\n",
      "6\n",
      "2\n",
      "'\n",
      "]\n",
      "[\n",
      "'\n",
      "G\n",
      "0\n",
      "6\n",
      "F\n",
      " \n",
      "1\n",
      "3\n",
      "/\n",
      "3\n",
      "8\n",
      "7\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "G\n",
      "0\n",
      "6\n",
      "F\n",
      " \n",
      "1\n",
      "3\n",
      "/\n",
      "3\n",
      "8\n",
      "7\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "H\n",
      "0\n",
      "4\n",
      "W\n",
      " \n",
      "4\n",
      "/\n",
      "8\n",
      "0\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "H\n",
      "0\n",
      "4\n",
      "W\n",
      " \n",
      "4\n",
      "/\n",
      "8\n",
      "0\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "G\n",
      "0\n",
      "6\n",
      "F\n",
      " \n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "/\n",
      "3\n",
      "8\n",
      "1\n",
      "4\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "G\n",
      "0\n",
      "6\n",
      "F\n",
      " \n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "/\n",
      "3\n",
      "8\n",
      "1\n",
      "4\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "Y\n",
      "0\n",
      "2\n",
      "D\n",
      " \n",
      "3\n",
      "0\n",
      "/\n",
      "7\n",
      "0\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "Y\n",
      "0\n",
      "2\n",
      "D\n",
      " \n",
      "3\n",
      "0\n",
      "/\n",
      "7\n",
      "0\n",
      "'\n",
      "]\n",
      "[\n",
      "'\n",
      "H\n",
      "0\n",
      "2\n",
      "J\n",
      " \n",
      "1\n",
      "3\n",
      "/\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "6\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "H\n",
      "0\n",
      "2\n",
      "J\n",
      " \n",
      "1\n",
      "3\n",
      "/\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "6\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "Y\n",
      "0\n",
      "2\n",
      "E\n",
      " \n",
      "6\n",
      "0\n",
      "/\n",
      "0\n",
      "0\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "Y\n",
      "0\n",
      "4\n",
      "S\n",
      " \n",
      "4\n",
      "0\n",
      "/\n",
      "1\n",
      "2\n",
      "4\n",
      "'\n",
      "]\n",
      "[\n",
      "'\n",
      "H\n",
      "0\n",
      "2\n",
      "M\n",
      " \n",
      "1\n",
      "/\n",
      "1\n",
      "0\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "H\n",
      "0\n",
      "2\n",
      "M\n",
      " \n",
      "1\n",
      "/\n",
      "1\n",
      "0\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "H\n",
      "0\n",
      "2\n",
      "M\n",
      " \n",
      "3\n",
      "/\n",
      "3\n",
      "3\n",
      "5\n",
      "9\n",
      "2\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "H\n",
      "0\n",
      "2\n",
      "M\n",
      " \n",
      "3\n",
      "/\n",
      "3\n",
      "3\n",
      "5\n",
      "9\n",
      "2\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "H\n",
      "0\n",
      "2\n",
      "M\n",
      " \n",
      "7\n",
      "/\n",
      "2\n",
      "1\n",
      "7\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "H\n",
      "0\n",
      "2\n",
      "M\n",
      " \n",
      "7\n",
      "/\n",
      "2\n",
      "1\n",
      "7\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "Y\n",
      "0\n",
      "2\n",
      "B\n",
      " \n",
      "7\n",
      "0\n",
      "/\n",
      "1\n",
      "0\n",
      "'\n",
      ",\n",
      " \n",
      "'\n",
      "Y\n",
      "0\n",
      "2\n",
      "B\n",
      " \n",
      "7\n",
      "0\n",
      "/\n",
      "1\n",
      "0\n",
      "'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for code in patents_text[\"cpc_codes_EPO\"].iloc[i]:\n",
    "        print(code)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"'(.*?)'\"\n",
    "\n",
    "patents_text[\"cpc_codes_EPO_list\"] = patents_text[\"cpc_codes_EPO\"].apply(lambda x: set(re.findall(pattern, x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19457"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unseen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data['title'] = unseen_data['title'].fillna('')\n",
    "unseen_data['abstract'] = unseen_data['abstract'].fillna('')\n",
    "unseen_data['description'] = unseen_data['description'].fillna('')\n",
    "unseen_data['claims'] = unseen_data['claims'].fillna('')\n",
    "\n",
    "# Function to extract first 200 words from a text\n",
    "def extract_first_300_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return ' '.join(words[:300])\n",
    "\n",
    "def extract_first_3000_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return ' '.join(words[:3000])\n",
    "\n",
    "# Apply the function to each row in training_data\n",
    "\n",
    "# Concatenate \"title\" and \"abstract\" columns, replacing NaN with empty string\n",
    "unseen_data[\"text_abstr\"] = (unseen_data[\"title\"] + \" \" + unseen_data[\"abstract\"]).fillna('')\n",
    "unseen_data[\"text_descr300\"] = (unseen_data[\"title\"] + \" \" + unseen_data[\"abstract\"] + \" \" + unseen_data[\"description\"].fillna('').apply(extract_first_300_words))\n",
    "unseen_data[\"text_descr\"] = (unseen_data[\"title\"] + \" \" + unseen_data[\"abstract\"] + \" \" + unseen_data[\"description\"]).fillna('')\n",
    "unseen_data[\"text_descr_claims\"] = (unseen_data[\"title\"] + \" \" + unseen_data[\"abstract\"] + \" \" + unseen_data[\"description\"] + \" \" + unseen_data[\"claims\"]).fillna('')\n",
    "unseen_data[\"text_descr3000_claims\"] = (unseen_data[\"title\"] + \" \" + unseen_data[\"abstract\"] + \" \" + unseen_data[\"description\"].fillna('').apply(extract_first_3000_words)) + \" \" + unseen_data[\"claims\"].fillna('')\n",
    "unseen_data[\"text_claims\"] = (unseen_data[\"title\"] + \" \" + unseen_data[\"abstract\"] + \" \" + unseen_data[\"claims\"]).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19457"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unseen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unseen_data.to_csv(\"unseen_data_new.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
